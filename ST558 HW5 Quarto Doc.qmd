---
title: "ST558 HW5 Quarto Doc"
author: "Liam Flaherty"
format: html
editor: visual
---

#ST558 HW4: Liam Flaherty

## Task 1: Conceptual Questions

### Task 1, Question 1

1.  What is the purpose of using cross-validation when fitting a random forest model?

> In general, using cross-validation allows us to prevent overfitting when we don't have sufficient data to split into training and test data.

### Task 1, Question 2

2.  Describe the bagged tree algorithm.

> The bagged tree algorithm essentially samples with replacement from itself, then builds trees on each sample subset.

### Task 1, Question 3

3.  What is meant by a general linear model?

> A general linear model is one that has a continuous response variable and (potentially) continuous and categorical predictors. This stands in contrast to *generalized* linear models, which allow for responses that are non-normal.

### Task 1, Question 4

4.  When fitting a multiple linear regression model, what does adding an interaction term do? That is, what does it allow the model to differently as compared to when it is not included in the model?

> The interaction term in the model is used when the two variables have different effects when considered together. For example, may asprin has an effect on heart rate, and alchohol has an effect on heart rate, but the precence of both alchohol and asprin generate different effects.

### Task 1, Question 5

5.  Why do we split our data into a training and test set?

> We split the data into training and tests sets to evaluate our models and prevent overfitting. Whenever we add terms, our correlation can only improve. But more complicated models may suffer from overfitting. When comparing models, we generally select the model which does "best" (by way of a measurment such as MSE) on the *test* data.

## Task 2: Fitting Models

We are given some data on heart disease from <https://www4.stat.ncsu.edu/~online/datasets/heart.csv>. We first download the data locally, then store it as an R object as usual.

```{r}
library(tidyverse)

heart=read_csv("heart.csv", show_col_types = FALSE)
heart                             #get a glimpse of the data#
```

### Task 2, EDA Part 1

We want to get a quick glimpse of our data. We can see that there are no missing values, but there are some irregularities. For example, in our summary, we see that certain people have a cholesterol of zero, a resting bloop pressure of zero, and a max heart rate of just 60bpm (while 60 would be a fine *resting* heart rate, a *maximum* heart rate that low suggests death).

```{r}
sum(is.na(heart))                 #no missing values#
summary(heart)                    #normal summaries#
```

There are a couple choices when dealing with such implausible data. One is to insert "dummy data", taking the average of all other values of the same predictor (essentially trading off information on the covariance among the predictors in an effort to preserve other variables for the observation which might be useful). A simpler way would be to delete the observations entirely. We should be careful though. While a maximum heart rate below 100 seems implausible, we should keep in mind that the data is based around heart failure. Indeed, when we check the data, we see that nearly 8% of our observations have a max heart rates under 100-- probably too much to be errors. Similarly, we see over 20% of our data have a cholesterol of zero. Since the data has been collected from multiple sources, it might be the case that certain sources have not included cholesterol in their measurements. It seems extreme to drop the observations entirely, as the other variables for the subject might be legitimate. For now, we will leave the dataset alone, and tend to the heart rate and cholesterol questions when we get a clearer sense of what our model might look like.

```{r}
sum(heart$MaxHR<100)                 #71#
sum(heart$Cholesterol==0)            #172#
heart[which(heart$RestingBP==0),]

```

### Task 2, EDA Part 2

We would like to make some minor alterations to our dataset. First, we transform the binary response (the presence of heart disease) from numeric to factor using `tivdyverse`'s `mutate()`. Next, we remove the `ST_Slope` and old numeric heart disease variables from our tibble.

```{r}
heart=heart|>
  mutate(HeartDisease=as.factor(HeartDisease)) |>
  select(-ST_Slope)
```

### Task 2, EDA Part 3

We are going to be performing a k-Nearest Neighbors analysis to predict whether or not someone has heart disease. In general, we want all numeric predictors for such an analysis. Luckily, the `caret` package offers a way to transform our data.

```{r}
library(caret)

dummies=dummyVars(~ Sex + ChestPainType + RestingECG + ExerciseAngina,
                 data=heart)
dummydf=predict(dummies, newdata=heart)
heart=as_tibble(cbind(heart,dummydf))

heart
```

### Task 2, Split Data

We'd now like to split our data into a training and test set. We can do this with normal `BaseR` functions, `sample()`ing without replacement 80% of the rows of our full tibble for our training set, and then putting the remaining rows in our test set with `setdiff()`. Note that in both our training and test sets, we only select the numeric columns using `select_if()` from the `tidyverse`.

```{r}
set.seed(558)   #to make reproducible#
training=sample(1:nrow(heart), size=nrow(heart)*.8, replace=FALSE)
test=setdiff(1:nrow(heart), training)

heart_training=heart[training,] |> select_if(function(x) !is.character(x))
heart_test=heart[test,] |> select_if(function(x) !is.character(x))
```

### Task 2, kNN

With our split data in hand, we can now train our model. We first center and scale our predictors (with the `preProcess` arguement to `train()` from the `caret` package). We want to test different choices of $k$, so create a data frame of forty rows (one for each value of $k$ that we are testing) with the `tuneGrid` argument. These forty values of $k$ are tested with cross-validation. We use 10-fold cross-validation (i.e. 10 disjoint sets, with nine acting as a predictor and the final as a test set, for each of the 10 sets in turn), repeating the process three times for each $k$ in order to make our prediction more stable (this is done with the `trControl` arguement). The below shows that our best choice of $k$, when all numeric predictors are included, is a $k$ value of 19.

```{r}
myknn=train(HeartDisease ~.,
            data=heart_training,
            method="knn",
            trControl=trainControl(method="repeatedcv", number=10, repeats=3),
            preProcess=c("center", "scale"),
            tuneGrid=data.frame(k=1:40)
            )
myknn
```

We can check the accuracy of our knn against our test set. We use the `confusionMatrix()` function from the `caret` library to see that our overall accuracy is about 82%, with balanced accuracy for both people who actually have heart disease and actually don't have heart disease (about 45.8% of the data in our test set actually did not have heart disease, which we correctly predicted about 82.7% of the time, while the remaining 54.2% of our test set actually did have heart disease, which we correctly predicted about 81.5% if the time).

```{r}
confusionMatrix(myknn, newdata=heart_test)
```

### Task 2, Logistic Regression

Let's now try some different types of analysis. Since our response is binary (you either have heart disease or you don't) it makes sense to try out a logistic regression. We try a few different models below. We elect to chose the model with the lowest AIC, which happends to be our full model.

```{r}
log_model_full=train(HeartDisease ~., 
                data=heart_training,
                method="glm",
                family="binomial",
                preProcess=c("center", "scale"),
                trControl=trainControl(method="repeatedcv", number=10, repeats=3))

log_model_reduced=train(HeartDisease ~ Cholesterol + FastingBS + ExerciseAnginaN + MaxHR + Oldpeak + SexF, 
                data=heart_training,
                method="glm",
                family="binomial",
                preProcess=c("center", "scale"),
                trControl=trainControl(method="repeatedcv", number=10, repeats=3))


log_model_reduced_min=train(HeartDisease ~ Cholesterol + FastingBS + MaxHR, 
                data=heart_training,
                method="glm",
                family="binomial",
                preProcess=c("center", "scale"),
                trControl=trainControl(method="repeatedcv", number=10, repeats=3))

df=data.frame(model=c("full", "reduced", "minimal"), 
           AIC=c(summary(log_model_full)$aic, 
                 summary(log_model_reduced)$aic, 
                 summary(log_model_reduced_min)$aic))

df

summary(log_model_full)
```

We would like to test how well our selected model does on our test data. We elect to use the same `confusionMatrix()` function we tried with $k$-nn, and see that the model correctly predicts about 82% of the time. This is similar to the result we got from $k$-nn.

```{r}
confusionMatrix(log_model_full, newdata=heart_test)

```

### Task 2, Tree Models

Another type of analysis that might be of interest are classification trees. We will first try a single tree, then use ensemble methods like random forests and boosted trees. We elect to do a model with our full parameter set.

```{r}
mytree=train(HeartDisease~., 
             data=heart_training,
             method="rpart",
             preProcess=c("center", "scale"),
             trControl=trainControl(method="repeatedcv", number=10, repeats=3),
             tuneGrid=data.frame(cp=seq(from=0, to=0.1, by=0.001))
             )
mytree
```

We see the accuracy is slightly worse than our $k$-nn and logistic regression models.

```{r}
confusionMatrix(mytree, newdata=heart_test)
```

Maybe a random forest will perform better. The only alteration we need from the above is changing our `method` argument from `rpart` to `rf`, and our `tuneGrid` argument to `mtry`, ranging from 1 to our number of parameters.

```{r}
library(randomForest)

myrandomforest=train(HeartDisease~., 
             data=heart_training,
             method="rf",
             preProcess=c("center", "scale"),
             trControl=trainControl(method="repeatedcv", number=10, repeats=3),
             tuneGrid=data.frame(mtry=1:(ncol(heart_training)-1))
             )
myrandomforest

```

The accuracy with our random forest improves to about the level of our logistic regression and our $k$-nn algorithm.

```{r}
confusionMatrix(myrandomforest, newdata=heart_test)
```

Maybe boosting our results will improve it even further. To do so we change our method to `gbm` and tweak the parameters to our `tuneGrid` in line with the instructions given.

```{r}
library(gbm)

myboosted=train(HeartDisease~., 
             data=heart_training,
             method="gbm",
             preProcess=c("center", "scale"),
             trControl=trainControl(method="repeatedcv", number=10, repeats=3),
             tuneGrid=expand.grid(n.trees=c(25,50,100,200),
                                  interaction.depth=1:3,
                                  shrinkage=0.1,
                                  n.minobsinnode=10),
             verbose=FALSE
             )
myboosted
```

Indeed, we see it does offer a slight improvement; it is our best model!

```{r}
confusionMatrix(myboosted, newdata=heart_test)
```
